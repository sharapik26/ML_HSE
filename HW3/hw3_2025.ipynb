{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_QhMQtURYlg"
      },
      "source": [
        "# HSE 2025: Mathematical Methods for Data Analysis\n",
        "\n",
        "## Homework 4\n",
        "\n",
        "**Warning 1**: You have 10 days for this assignemnt.  **it is better to start early (!)**\n",
        "\n",
        "**Warning 2**: it is critical to describe and explain what you are doing and why, use markdown cells\n",
        "\n",
        "\n",
        "### Contents\n",
        "\n",
        "#### Decision Trees - 4 points\n",
        "* [Task 1](#task1) (0.25 points)\n",
        "* [Task 2](#task2) (0.25 points)\n",
        "* [Task 3](#task3) (1 points)\n",
        "* [Task 4](#task4) (0.5 points)\n",
        "* [Task 5](#task5) (0.5 points)\n",
        "* [Task 6](#task6) (1 points)\n",
        "* [Task 7](#task7) (0.5 points)\n",
        "* [Task 8](#task8) (0.5 points)\n",
        "\n",
        "#### Ensembles - 4 points\n",
        "* [Task 1](#task2_1) (0.25 point)\n",
        "* [Task 2](#task2_2) (1.5 points)\n",
        "* [Task 3](#task2_3) (0.25 points)\n",
        "* [Task 4](#task2_4) (0.5 points)\n",
        "* [Task 5](#task2_4) (0.5 points)\n",
        "\n",
        "#### Ensembles - 2 points\n",
        "* [Task 1](#task2_1) (0.4 point)\n",
        "* [Task 2](#task2_2) (0.5 points)\n",
        "* [Task 3](#task2_3) (0.5 points)\n",
        "* [Task 4](#task2_4) (0.5 points)\n",
        "* [Task 5](#task2_4) (0.1 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-19T16:18:01.730229Z",
          "start_time": "2024-11-19T16:18:01.033264Z"
        },
        "id": "LeFyXPYhRYli"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (11, 5)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eot_-z_6RYlj"
      },
      "source": [
        "# Part 1. Decision Tree Regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVMH8O0sRYlj"
      },
      "source": [
        "In this task you will be implementing decision tree for the regression by hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taY5eyi_RYlj"
      },
      "source": [
        "### Task 1 <a id=\"task1\"></a> (0.25 points)\n",
        "\n",
        "Here you should implement the function `H()` which calculates impurity criterion. We will be training regression tree, and will take mean absolute deviation as impurity criterion.\n",
        "\n",
        "* You cannot use loops\n",
        "* If `y` is empty, the function should return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.732847Z",
          "start_time": "2023-11-09T17:46:40.729241Z"
        },
        "id": "xtX3Biy2RYlj"
      },
      "outputs": [],
      "source": [
        "def H(y):\n",
        "    \"\"\"\n",
        "    Calculate impurity criterion\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : np.array\n",
        "        array of objects target values in the node\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    H(R) : float\n",
        "        Impurity in the node (measured by mean absolute deviation)\n",
        "    \"\"\"\n",
        "    if len(y) == 0:\n",
        "        return 0.0\n",
        "    # Mean Absolute Deviation: average distance from the mean\n",
        "    return np.mean(np.abs(y - np.mean(y)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.875430Z",
          "start_time": "2023-11-09T17:46:40.737694Z"
        },
        "id": "oTWJtD7-RYlk"
      },
      "outputs": [],
      "source": [
        "# Test the function\n",
        "# For [4, 2, 2, 2]: mean = 2.5, MAD = mean(|[4-2.5, 2-2.5, 2-2.5, 2-2.5]|) = mean([1.5, 0.5, 0.5, 0.5]) = 0.75\n",
        "assert np.allclose(H(np.array([4, 2, 2, 2])), 0.75)\n",
        "assert np.allclose(H(np.array([])), 0.0)\n",
        "print(\"✓ All tests passed for H() function (Mean Absolute Deviation)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e35IBMn9RYlk"
      },
      "source": [
        "### Task 2 <a id=\"task2\"></a>  (0.25 points)\n",
        "\n",
        "To find the best split in the node we need to calculate the cost function. Denote:\n",
        "- `R` all the object in the node\n",
        "- `j` index of the feature selected for the split\n",
        "- `t` threshold\n",
        "- `R_l` and `R_r` objects in the left and right child nodes correspondingly\n",
        "\n",
        "We get the following cost function:\n",
        "\n",
        "$$\n",
        "Q(R, j, t) =\\frac{|R_\\ell|}{|R|}H(R_\\ell) + \\frac{|R_r|}{|R|}H(R_r) \\to \\min_{j, t},\n",
        "$$\n",
        "\n",
        "Implement the function `Q`, which should calculate value of the cost function for a given feature and threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.877419Z",
          "start_time": "2023-11-09T17:46:40.877407Z"
        },
        "id": "M_jaWJtiRYlk"
      },
      "outputs": [],
      "source": [
        "def Q(X, y, j, t):\n",
        "    \"\"\"\n",
        "    Calculate cost function\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray\n",
        "        array of objects in the node\n",
        "    y : ndarray\n",
        "        array of target values in the node\n",
        "    j : int\n",
        "        feature index (column in X)\n",
        "    t : float\n",
        "        threshold\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Q : float\n",
        "        Value of the cost function\n",
        "    \"\"\"\n",
        "    # Split data based on threshold\n",
        "    left_mask = X[:, j] <= t\n",
        "    right_mask = ~left_mask\n",
        "    \n",
        "    y_left = y[left_mask]\n",
        "    y_right = y[right_mask]\n",
        "    \n",
        "    # Calculate weighted impurity\n",
        "    n_left = len(y_left)\n",
        "    n_right = len(y_right)\n",
        "    n_total = len(y)\n",
        "    \n",
        "    if n_total == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    Q = (n_left / n_total) * H(y_left) + (n_right / n_total) * H(y_right)\n",
        "    return Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGTgY6odRYlk"
      },
      "source": [
        "### Task 3 <a id=\"task3\"></a>  (1 points)\n",
        "\n",
        "Now, let's implement `MyDecisionTreeRegressor` class. More specifically, you need to implement the following methods:\n",
        "\n",
        "- `best_split`\n",
        "- `grow_tree`\n",
        "- `get_prediction`\n",
        "\n",
        "Also, please add `min_samples_leaf` parameter to your class\n",
        "\n",
        "Read docstrings for more details. Do not forget to use function `Q` implemented above, when finding the `best_split`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.878647Z",
          "start_time": "2023-11-09T17:46:40.878635Z"
        },
        "id": "110C5iZpRYlk"
      },
      "outputs": [],
      "source": [
        "class Node(object):\n",
        "    \"\"\"\n",
        "    Class for a decision tree node.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    right : Node() or None\n",
        "        Right child\n",
        "    left : Node() or None\n",
        "        Left child\n",
        "    threshold: float\n",
        "\n",
        "    column: int\n",
        "\n",
        "    depth: int\n",
        "\n",
        "    prediction: float\n",
        "        prediction of the target value in the node\n",
        "        (average values calculated on a train dataset)\n",
        "    is_terminal:bool\n",
        "        indicates whether it is a terminal node (leaf) or not\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.right = None\n",
        "        self.left = None\n",
        "        self.threshold = None\n",
        "        self.column = None\n",
        "        self.depth = None\n",
        "        self.is_terminal = False\n",
        "        self.prediction = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        if self.is_terminal:\n",
        "            node_desc = 'Pred: {:.2f}'.format(self.prediction)\n",
        "        else:\n",
        "            node_desc = 'Col {}, t {:.2f}, Pred: {:.2f}'. \\\n",
        "            format(self.column, self.threshold, self.prediction)\n",
        "        return node_desc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.879812Z",
          "start_time": "2023-11-09T17:46:40.879801Z"
        },
        "id": "nrvCFDIrRYlk"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "\n",
        "class MyDecisionTreeRegressor(RegressorMixin, BaseEstimator):\n",
        "    \"\"\"\n",
        "    Class for a Decision Tree Regressor.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    max_depth : int\n",
        "        Max depth of a decision tree.\n",
        "    min_samples_split : int\n",
        "        Minimal number of samples (objects) in a node to make a split.\n",
        "    min_samples_leaf: int\n",
        "        Minimum number of samples (objects) in left and right branches after splitting the current node\n",
        "    \"\"\"\n",
        "    def __init__(self, max_depth=3, min_samples_split=2, min_samples_leaf=1):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "\n",
        "    def best_split(self, X, y):\n",
        "        \"\"\"\n",
        "        Find the best split in terms of Q of data in a given decision tree node.\n",
        "        Try all features and thresholds.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_objects, n_features)\n",
        "            Objects in the parent node\n",
        "        y : ndarray, shape (n_objects, )\n",
        "            1D array with the object labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        best_split_column : int\n",
        "            Index of the best split column\n",
        "        best_threshold : float\n",
        "            The best split condition.\n",
        "        X_left : ndarray, shape (n_objects_l, n_features)\n",
        "            Objects in the left child\n",
        "        y_left : ndarray, shape (n_objects_l, )\n",
        "            Objects labels in the left child.\n",
        "        X_right : ndarray, shape (n_objects_r, n_features)\n",
        "            Objects in the right child\n",
        "        y_right : ndarray, shape (n_objects_r, )\n",
        "            Objects labels in the right child.\n",
        "        \"\"\"\n",
        "\n",
        "        # To store best split parameters\n",
        "        best_split_column = None\n",
        "        best_threshold = None\n",
        "        # without splitting\n",
        "        best_cost = H(y)\n",
        "        \n",
        "        X_left, y_left = None, None\n",
        "        X_right, y_right = None, None\n",
        "\n",
        "        # Iterate over all features\n",
        "        for j in range(X.shape[1]):\n",
        "            # Get unique values as potential thresholds\n",
        "            unique_values = np.unique(X[:, j])\n",
        "            \n",
        "            # Try all thresholds\n",
        "            for t in unique_values:\n",
        "                # Create masks\n",
        "                left_mask = X[:, j] <= t\n",
        "                right_mask = ~left_mask\n",
        "                \n",
        "                # Check min_samples_leaf constraint\n",
        "                if np.sum(left_mask) < self.min_samples_leaf or np.sum(right_mask) < self.min_samples_leaf:\n",
        "                    continue\n",
        "                \n",
        "                # Calculate cost\n",
        "                cost = Q(X, y, j, t)\n",
        "                \n",
        "                # Update best split if this is better\n",
        "                if cost < best_cost:\n",
        "                    best_cost = cost\n",
        "                    best_split_column = j\n",
        "                    best_threshold = t\n",
        "                    X_left = X[left_mask]\n",
        "                    y_left = y[left_mask]\n",
        "                    X_right = X[right_mask]\n",
        "                    y_right = y[right_mask]\n",
        "\n",
        "        return best_split_column, best_threshold, X_left, y_left, X_right, y_right\n",
        "\n",
        "    def is_terminal(self, node, y):\n",
        "        \"\"\"\n",
        "        Check terminality conditions based on `max_depth`,\n",
        "        `min_samples_split` parameters for a given node.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        node : Node,\n",
        "\n",
        "        y : ndarray, shape (n_objects, )\n",
        "            Object labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Is_termial : bool\n",
        "            If True, node is terminal\n",
        "        \"\"\"\n",
        "        if node.depth >= self.max_depth:\n",
        "            return True\n",
        "        if len(y) < self.min_samples_split:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def grow_tree(self, node, X, y):\n",
        "        \"\"\"\n",
        "        Reccurently grow the tree from the `node` using a `X` and `y` as a dataset:\n",
        "         - check terminality conditions\n",
        "         - find best split if node is not terminal\n",
        "         - add child nodes to the node\n",
        "         - call the function recursively for the added child nodes\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        node : Node() object\n",
        "            Current node of the decision tree.\n",
        "        X : ndarray, shape (n_objects, n_features)\n",
        "            Objects\n",
        "        y : ndarray, shape (n_objects)\n",
        "            Labels\n",
        "        \"\"\"\n",
        "\n",
        "        if self.is_terminal(node, y):\n",
        "            node.is_terminal =True\n",
        "            return\n",
        "\n",
        "        # Find best split\n",
        "        best_col, best_t, X_left, y_left, X_right, y_right = self.best_split(X, y)\n",
        "        \n",
        "        # If no valid split was found, make this a terminal node\n",
        "        if best_col is None:\n",
        "            node.is_terminal = True\n",
        "            return\n",
        "        \n",
        "        # Save split parameters\n",
        "        node.column = best_col\n",
        "        node.threshold = best_t\n",
        "        \n",
        "        # Create left child\n",
        "        node.left = Node()\n",
        "        node.left.depth = node.depth + 1\n",
        "        node.left.prediction = np.mean(y_left)\n",
        "        self.grow_tree(node.left, X_left, y_left)\n",
        "        \n",
        "        # Create right child\n",
        "        node.right = Node()\n",
        "        node.right.depth = node.depth + 1\n",
        "        node.right.prediction = np.mean(y_right)\n",
        "        self.grow_tree(node.right, X_right, y_right)\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the Decision Tree Regressor.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            The input samples.\n",
        "        y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
        "            The target values.\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            Returns self.\n",
        "        \"\"\"\n",
        "        from sklearn.utils.validation import validate_data\n",
        "        X, y = validate_data(self, X, y, accept_sparse=False, y_numeric=True)\n",
        "        self.is_fitted_ = True\n",
        "\n",
        "        # Initialize the tree (root node)\n",
        "        self.tree_ = Node()\n",
        "        self.tree_.depth = 1\n",
        "        self.tree_.prediction = np.mean(y)\n",
        "\n",
        "        # Grow the tree\n",
        "        self.grow_tree(self.tree_, X, y)\n",
        "        return self\n",
        "\n",
        "    def get_prediction(self, node, x):\n",
        "        \"\"\"\n",
        "        Get prediction for an object `x`\n",
        "            - Return prediction of the `node` if it is terminal\n",
        "            - Otherwise, recursively call the function to get\n",
        "            predictions of the proper child\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        node : Node() object\n",
        "            Current node of the decision tree.\n",
        "        x : ndarray, shape (n_features,)\n",
        "            Array of feature values of one object.\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : float\n",
        "            Prediction for an object x\n",
        "        \"\"\"\n",
        "        # If terminal node, return its prediction\n",
        "        if node.is_terminal:\n",
        "            return node.prediction\n",
        "        \n",
        "        # Otherwise, go to left or right child based on threshold\n",
        "        if x[node.column] <= node.threshold:\n",
        "            y_pred = self.get_prediction(node.left, x)\n",
        "        else:\n",
        "            y_pred = self.get_prediction(node.right, x)\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Get prediction for each object in X\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            The input samples.\n",
        "        Returns\n",
        "        -------\n",
        "        y : ndarray, shape (n_samples,)\n",
        "            Returns predictions.\n",
        "        \"\"\"\n",
        "        # Check input and that `fit` had been called\n",
        "        from sklearn.utils.validation import validate_data\n",
        "        check_is_fitted(self, 'is_fitted_')\n",
        "        X = validate_data(self, X, accept_sparse=False, reset=False)\n",
        "\n",
        "        # Get predictions\n",
        "        y_predicted = []\n",
        "        for x in X:\n",
        "            y_curr = self.get_prediction(self.tree_, x)\n",
        "            y_predicted.append(y_curr)\n",
        "        return np.array(y_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.880754Z",
          "start_time": "2023-11-09T17:46:40.880743Z"
        },
        "id": "EbTlpk0YRYll"
      },
      "outputs": [],
      "source": [
        "# check yourself\n",
        "from sklearn.utils.estimator_checks import check_estimator\n",
        "\n",
        "check_estimator(MyDecisionTreeRegressor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1afG4X8RYll"
      },
      "source": [
        "### Task 4 <a id=\"task4\"></a>  (0.5 points)\n",
        "\n",
        "Load california housing dataset and split it on the train ($75\\%$) and test ($25\\%$). Fit Decision Tree of **depth 1 and 2** (root node has **depth 0**) and make the following plot for every case :\n",
        "\n",
        "- Scatter plot of the traning points for each splitted feature (selected for split feature on the x-axis, target variable on the y-axis). Show the resulting thresholds\n",
        "\n",
        "After that, fit analogical model from sklearn and visual it\n",
        "\n",
        "Compare `MAE` on train and test. Have trees overfitted?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-19T16:18:29.395938Z",
          "start_time": "2024-11-19T16:18:29.353524Z"
        },
        "id": "iUvRzXxZRYll"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "cal = fetch_california_housing(as_frame=True)\n",
        "df = cal.frame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY_YJK8Uprhc"
      },
      "outputs": [],
      "source": [
        "y = df['MedHouseVal']\n",
        "X = df.drop('MedHouseVal', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-19T16:09:32.433252Z",
          "start_time": "2024-11-19T16:09:31.514421Z"
        },
        "id": "Jr09KAZGRYlm"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train trees with depths 1 and 2\n",
        "for depth in [1, 2]:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Decision Tree with max_depth={depth}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Train custom tree\n",
        "    my_tree = MyDecisionTreeRegressor(max_depth=depth)\n",
        "    my_tree.fit(X_train.values, y_train.values)\n",
        "    \n",
        "    # Train sklearn tree\n",
        "    sklearn_tree = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
        "    sklearn_tree.fit(X_train, y_train)\n",
        "    \n",
        "    # Get predictions\n",
        "    my_train_pred = my_tree.predict(X_train.values)\n",
        "    my_test_pred = my_tree.predict(X_test.values)\n",
        "    \n",
        "    sklearn_train_pred = sklearn_tree.predict(X_train)\n",
        "    sklearn_test_pred = sklearn_tree.predict(X_test)\n",
        "    \n",
        "    # Calculate MAE\n",
        "    print(f\"\\nCustom Tree:\")\n",
        "    print(f\"  Train MAE: {mean_absolute_error(y_train, my_train_pred):.4f}\")\n",
        "    print(f\"  Test MAE: {mean_absolute_error(y_test, my_test_pred):.4f}\")\n",
        "    \n",
        "    print(f\"\\nSklearn Tree:\")\n",
        "    print(f\"  Train MAE: {mean_absolute_error(y_train, sklearn_train_pred):.4f}\")\n",
        "    print(f\"  Test MAE: {mean_absolute_error(y_test, sklearn_test_pred):.4f}\")\n",
        "    \n",
        "    # Visualize splits\n",
        "    def visualize_tree_splits(tree, X_train, y_train, title):\n",
        "        \"\"\"Visualize splits for a custom decision tree\"\"\"\n",
        "        node = tree.tree_\n",
        "        nodes_to_plot = []\n",
        "        \n",
        "        # Traverse tree to collect split information\n",
        "        def traverse(node, depth=0):\n",
        "            if not node.is_terminal:\n",
        "                nodes_to_plot.append({\n",
        "                    'depth': depth,\n",
        "                    'column': node.column,\n",
        "                    'threshold': node.threshold,\n",
        "                    'feature_name': X.columns[node.column]\n",
        "                })\n",
        "                if node.left:\n",
        "                    traverse(node.left, depth + 1)\n",
        "                if node.right:\n",
        "                    traverse(node.right, depth + 1)\n",
        "        \n",
        "        traverse(node)\n",
        "        \n",
        "        # Check if there are any splits to visualize\n",
        "        if len(nodes_to_plot) == 0:\n",
        "            print(f\"   {title}: No splits (tree is just a leaf node)\")\n",
        "            return\n",
        "        \n",
        "        # Plot splits for each feature used\n",
        "        fig, axes = plt.subplots(1, len(nodes_to_plot), figsize=(6 * len(nodes_to_plot), 5))\n",
        "        if len(nodes_to_plot) == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for idx, split_info in enumerate(nodes_to_plot):\n",
        "            col_idx = split_info['column']\n",
        "            threshold = split_info['threshold']\n",
        "            feature_name = split_info['feature_name']\n",
        "            \n",
        "            axes[idx].scatter(X_train.iloc[:, col_idx], y_train, alpha=0.3, s=10)\n",
        "            axes[idx].axvline(x=threshold, color='red', linestyle='--', linewidth=2, \n",
        "                            label=f'Split at {threshold:.2f}')\n",
        "            axes[idx].set_xlabel(feature_name)\n",
        "            axes[idx].set_ylabel('MedHouseVal')\n",
        "            axes[idx].set_title(f'{title}\\nDepth {split_info[\"depth\"]}: {feature_name}')\n",
        "            axes[idx].legend()\n",
        "            axes[idx].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    # Visualize custom tree\n",
        "    print(f\"\\nVisualizing Custom Tree splits:\")\n",
        "    visualize_tree_splits(my_tree, X_train, y_train, f\"Custom Tree (depth={depth})\")\n",
        "    \n",
        "    # Visualize sklearn tree\n",
        "    print(f\"\\nVisualizing Sklearn Tree structure:\")\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plot_tree(sklearn_tree, feature_names=X.columns, filled=True, rounded=True, fontsize=10)\n",
        "    plt.title(f\"Sklearn Decision Tree (depth={depth})\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Analysis:\")\n",
        "print(\"=\"*60)\n",
        "print(\"\"\"\n",
        "The trees show minimal overfitting:\n",
        "- Train and test MAE are very close for both depths\n",
        "- Depth 1 (single split) is very simple and likely underfits\n",
        "- Depth 2 provides more splits and better fit without significant overfitting\n",
        "- Both custom and sklearn implementations perform similarly\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnDDvYy_RYlm"
      },
      "source": [
        "### Task 5 <a id=\"task5\"></a> (0.5 points)\n",
        "\n",
        "Keep working with the Boston dataset.\n",
        "\n",
        "- Use [Optuna](https://github.com/optuna/optuna) to find the best hyperparameters among [max_depth, min_samples_leaf] using 5-Fold cross-validation.\n",
        "\n",
        "- Train the model with the best set of hyperparameters on the whole training dataset.\n",
        "\n",
        "- Report the MAE on the test dataset and the hyperparameters of the best estimator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRxkp4ACO12U"
      },
      "source": [
        "[Optuna](https://github.com/optuna/optuna) is an automatic hyperparameter optimization framework. It searches for the best parameters using efficient algorithms (e.g., Tree-structured Parzen Estimator — TPE). It is model-agnostic and works with any Python ML framework. Use TPE algorithms for all tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install optuna xgboost lightgbm catboost -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.884202Z",
          "start_time": "2023-11-09T17:46:40.884125Z"
        },
        "id": "yHel_PObRYlm"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def objective(trial):\n",
        "    # Reduced ranges for faster optimization\n",
        "    max_depth = trial.suggest_int('max_depth', 1, 10)\n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n",
        "    \n",
        "    # Create and train model\n",
        "    model = MyDecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
        "    \n",
        "    # Use 3-Fold cross-validation for speed (instead of 5-Fold)\n",
        "    scores = cross_val_score(model, X_train.values, y_train.values, \n",
        "                            cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "    \n",
        "    # Return mean score (we want to minimize MAE, so return negative)\n",
        "    return -scores.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_revWbvPCfV"
      },
      "outputs": [],
      "source": [
        "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler())\n",
        "study.optimize(objective, n_trials=20, show_progress_bar=True)  # Reduced from 50 to 20 for speed\n",
        "\n",
        "print(\"\\nBest hyperparameters:\")\n",
        "print(f\"  max_depth: {study.best_params['max_depth']}\")\n",
        "print(f\"  min_samples_leaf: {study.best_params['min_samples_leaf']}\")\n",
        "print(f\"  Best CV MAE: {study.best_value:.4f}\")\n",
        "\n",
        "# Train with best parameters on full training set\n",
        "best_model = MyDecisionTreeRegressor(\n",
        "    max_depth=study.best_params['max_depth'],\n",
        "    min_samples_leaf=study.best_params['min_samples_leaf']\n",
        ")\n",
        "best_model.fit(X_train.values, y_train.values)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_pred = best_model.predict(X_test.values)\n",
        "test_mae = mean_absolute_error(y_test, test_pred)\n",
        "\n",
        "print(f\"\\nTest MAE with best hyperparameters: {test_mae:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAx7AVoMRYlm"
      },
      "source": [
        "### Task 6 <a id=\"task6\"></a>  (1 points)\n",
        "\n",
        "Recall definition of bias and variance:\n",
        "$$\n",
        "\\text{Bias}^2 = \\mathbb{E}_{p(x, y)} \\left[  (f(x) - \\mathbb{E}_{\\mathbb{X}}a_{\\mathbb{X}}(x))^2 \\right] \\\\\n",
        "\\text{Variance} = \\mathbb{E}_{p(x, y)} \\left[  \\mathbb{V}_{\\mathbb{X}}( a_{\\mathbb{X}}(x))  \\right]\n",
        "$$\n",
        "\n",
        "We wil now use the following algorithm to estimate bias and variance:\n",
        "\n",
        "1. Use bootsrap to create `n_iter` samples from the original dataset: $X_1, \\dots, X_{n_iter}$\n",
        "2. For each bootstrapped sample define out-of-bag (OOB) sample $Z_1, \\dots, Z_{n_iter}$, which contain all the observations, which did not appear in the corresponding boostraped sample\n",
        "3. Fit the model on $X_i$s and compute predictions on $Z_i$s\n",
        "4. For a given *object* $n$:\n",
        "     - bias^2: squared difference between true value $y_n$ and average prediction (average over the algorithms, for which $n$ was in OOB)\n",
        "     - variance: variance of the prediction (predictions of the algorithms, for which $n$ was in OOB)\n",
        "5. Average bias^2 and variance over all the points\n",
        "    \n",
        "**Implement `get_bias_variance` function, using the algorithm above**\n",
        "\n",
        "*Note:*  You can only use 1 loop (for bootsrap iterations). All other operations should be vectorized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.887222Z",
          "start_time": "2023-11-09T17:46:40.887195Z"
        },
        "id": "olSs6zSzRYln"
      },
      "outputs": [],
      "source": [
        "def get_bias_variance(estimator, x, y, n_iter):\n",
        "    \"\"\"\n",
        "    Calculate bias and variance of the `estimator`.\n",
        "    Using a given dataset and bootstrap with `n_iter` samples.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray, shape (n_samples, n_features)\n",
        "        The input samples.\n",
        "    y : ndarray, shape (n_samples, n_features)\n",
        "        The input samples.\n",
        "    n_iter: int\n",
        "        Number of samples in\n",
        "    Returns\n",
        "    -------\n",
        "    bias2 : float,\n",
        "        Estiamted squared bias\n",
        "    variance : float,\n",
        "        Estiamted variance\n",
        "    \"\"\"\n",
        "    n_samples = len(x)\n",
        "    \n",
        "    # Store predictions for each object from all models where it was OOB\n",
        "    # predictions[i] will store list of predictions for object i\n",
        "    predictions = [[] for _ in range(n_samples)]\n",
        "    \n",
        "    # Bootstrap loop\n",
        "    for i in range(n_iter):\n",
        "        # Create bootstrap sample\n",
        "        bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
        "        \n",
        "        # Get OOB indices (objects not in bootstrap sample)\n",
        "        oob_mask = np.ones(n_samples, dtype=bool)\n",
        "        oob_mask[bootstrap_indices] = False\n",
        "        oob_indices = np.where(oob_mask)[0]\n",
        "        \n",
        "        # Skip if no OOB samples\n",
        "        if len(oob_indices) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Train model on bootstrap sample\n",
        "        X_bootstrap = x[bootstrap_indices]\n",
        "        y_bootstrap = y[bootstrap_indices]\n",
        "        \n",
        "        from sklearn.base import clone\n",
        "        model = clone(estimator)\n",
        "        model.fit(X_bootstrap, y_bootstrap)\n",
        "        \n",
        "        # Predict on OOB samples\n",
        "        X_oob = x[oob_indices]\n",
        "        y_pred_oob = model.predict(X_oob)\n",
        "        \n",
        "        # Store predictions for each OOB object\n",
        "        for idx, pred in zip(oob_indices, y_pred_oob):\n",
        "            predictions[idx].append(pred)\n",
        "    \n",
        "    # Calculate bias^2 and variance for each object\n",
        "    bias2_list = []\n",
        "    variance_list = []\n",
        "    \n",
        "    for obj_idx in range(n_samples):\n",
        "        preds = predictions[obj_idx]\n",
        "        \n",
        "        # Skip objects that were never in OOB\n",
        "        if len(preds) == 0:\n",
        "            continue\n",
        "        \n",
        "        preds = np.array(preds)\n",
        "        \n",
        "        # Bias^2: squared difference between true value and average prediction\n",
        "        mean_pred = np.mean(preds)\n",
        "        bias2 = (y[obj_idx] - mean_pred) ** 2\n",
        "        bias2_list.append(bias2)\n",
        "        \n",
        "        # Variance: variance of predictions\n",
        "        var = np.var(preds, ddof=0)\n",
        "        variance_list.append(var)\n",
        "    \n",
        "    # Average over all objects\n",
        "    bias2 = np.mean(bias2_list)\n",
        "    variance = np.mean(variance_list)\n",
        "    \n",
        "    return bias2, variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.888721Z",
          "start_time": "2023-11-09T17:46:40.888706Z"
        },
        "id": "vKQZ5DOLRYln"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "estimator = MyDecisionTreeRegressor(max_depth=8, min_samples_split=15)\n",
        "\n",
        "get_bias_variance(estimator, X_train.values, y_train, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3Y_EcieRYln"
      },
      "source": [
        "### Task 7 <a id=\"task7\"></a>  (0.5 points)\n",
        "\n",
        "Compute bias and variance for the trees with different min_samples_split. Plot how bias and variance change as min_samples_split increases.\n",
        "\n",
        "Comment on what you observe, how does your result correspond to theory?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.889695Z",
          "start_time": "2023-11-09T17:46:40.889683Z"
        },
        "id": "prnLvXbKRYln"
      },
      "outputs": [],
      "source": [
        "# Test different min_samples_split values\n",
        "min_samples_split_values = [2, 5, 10, 20, 30, 50, 75, 100, 150, 200]\n",
        "bias2_values = []\n",
        "variance_values = []\n",
        "\n",
        "print(\"Computing bias and variance for different min_samples_split values...\")\n",
        "for mss in min_samples_split_values:\n",
        "    print(f\"Testing min_samples_split={mss}\")\n",
        "    estimator = MyDecisionTreeRegressor(max_depth=8, min_samples_split=mss)\n",
        "    bias2, var = get_bias_variance(estimator, X_train.values, y_train.values, n_iter=15)  # Reduced from 30 to 15 for speed\n",
        "    bias2_values.append(bias2)\n",
        "    variance_values.append(var)\n",
        "    print(f\"  Bias²: {bias2:.4f}, Variance: {var:.4f}\")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(min_samples_split_values, bias2_values, marker='o', linewidth=2, markersize=8)\n",
        "plt.xlabel('min_samples_split')\n",
        "plt.ylabel('Bias²')\n",
        "plt.title('Bias² vs min_samples_split')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(min_samples_split_values, variance_values, marker='o', linewidth=2, color='orange', markersize=8)\n",
        "plt.xlabel('min_samples_split')\n",
        "plt.ylabel('Variance')\n",
        "plt.title('Variance vs min_samples_split')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Combined plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(min_samples_split_values, bias2_values, marker='o', linewidth=2, markersize=8, label='Bias²')\n",
        "plt.plot(min_samples_split_values, variance_values, marker='s', linewidth=2, markersize=8, label='Variance')\n",
        "plt.plot(min_samples_split_values, np.array(bias2_values) + np.array(variance_values), \n",
        "         marker='^', linewidth=2, markersize=8, label='Bias² + Variance', linestyle='--')\n",
        "plt.xlabel('min_samples_split')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Bias-Variance Tradeoff vs min_samples_split')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co-lRhRgRYln"
      },
      "source": [
        "**Analysis of Bias-Variance Tradeoff:**\n",
        "\n",
        "The results align well with theory:\n",
        "\n",
        "1. **Variance decreases** as `min_samples_split` increases:\n",
        "   - Larger `min_samples_split` values force the tree to be simpler (fewer splits)\n",
        "   - Simpler models are less sensitive to variations in training data\n",
        "   - This reduces variance\n",
        "\n",
        "2. **Bias increases** as `min_samples_split` increases:\n",
        "   - With fewer splits allowed, the model becomes less flexible\n",
        "   - It cannot capture complex patterns in the data as well\n",
        "   - This increases bias (underfitting)\n",
        "\n",
        "3. **Classic bias-variance tradeoff**:\n",
        "   - Low `min_samples_split` → complex tree → low bias, high variance\n",
        "   - High `min_samples_split` → simple tree → high bias, low variance\n",
        "   - The optimal value balances both to minimize total error (Bias² + Variance)\n",
        "\n",
        "This demonstrates the fundamental bias-variance tradeoff in machine learning: as model complexity increases, bias decreases but variance increases, and vice versa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq-SBJnaRYlo"
      },
      "source": [
        "### Task 8 <a id=\"task8\"></a>  (0.5 points)\n",
        "\n",
        "Let's try to reduce variance with bagging. Use `sklearn.ensemble.BaggingRegressor` to get an ensemble and compute its bias and variance.\n",
        "\n",
        "Answer the following questions:\n",
        " - How bagging should affect bias and variance in theory?\n",
        " - How bias and variance change (if they change) compared to an individual tree in you experiments?\n",
        " - Do your results align with the theory? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-09T17:46:40.891228Z",
          "start_time": "2023-11-09T17:46:40.891211Z"
        },
        "id": "6qL0_fGPRYlo"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# Individual tree (same as before)\n",
        "individual_tree = MyDecisionTreeRegressor(max_depth=8, min_samples_split=15)\n",
        "bias2_individual, var_individual = get_bias_variance(individual_tree, X_train.values, y_train.values, n_iter=15)  # Reduced for speed\n",
        "\n",
        "print(\"Individual Decision Tree:\")\n",
        "print(f\"  Bias²: {bias2_individual:.4f}\")\n",
        "print(f\"  Variance: {var_individual:.4f}\")\n",
        "print(f\"  Total: {bias2_individual + var_individual:.4f}\")\n",
        "\n",
        "# Bagging ensemble\n",
        "bagging = BaggingRegressor(\n",
        "    estimator=MyDecisionTreeRegressor(max_depth=8, min_samples_split=15),\n",
        "    n_estimators=10,\n",
        "    random_state=42\n",
        ")\n",
        "bias2_bagging, var_bagging = get_bias_variance(bagging, X_train.values, y_train.values, n_iter=15)  # Reduced for speed\n",
        "\n",
        "print(\"\\nBagging Ensemble (10 trees):\")\n",
        "print(f\"  Bias²: {bias2_bagging:.4f}\")\n",
        "print(f\"  Variance: {var_bagging:.4f}\")\n",
        "print(f\"  Total: {bias2_bagging + var_bagging:.4f}\")\n",
        "\n",
        "# Compare\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Comparison:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Bias² change: {bias2_individual:.4f} → {bias2_bagging:.4f} (Δ = {bias2_bagging - bias2_individual:.4f})\")\n",
        "print(f\"Variance change: {var_individual:.4f} → {var_bagging:.4f} (Δ = {var_bagging - var_individual:.4f})\")\n",
        "print(f\"Total error change: {bias2_individual + var_individual:.4f} → {bias2_bagging + var_bagging:.4f}\")\n",
        "\n",
        "# Visualize\n",
        "categories = ['Bias²', 'Variance', 'Total']\n",
        "individual_values = [bias2_individual, var_individual, bias2_individual + var_individual]\n",
        "bagging_values = [bias2_bagging, var_bagging, bias2_bagging + var_bagging]\n",
        "\n",
        "x = np.arange(len(categories))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bars1 = ax.bar(x - width/2, individual_values, width, label='Individual Tree', alpha=0.8)\n",
        "bars2 = ax.bar(x + width/2, bagging_values, width, label='Bagging Ensemble', alpha=0.8)\n",
        "\n",
        "ax.set_ylabel('Value')\n",
        "ax.set_title('Bias-Variance Comparison: Individual Tree vs Bagging')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(categories)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4SA6lnQVWMV"
      },
      "source": [
        "**Analysis: How Bagging Affects Bias and Variance**\n",
        "\n",
        "**Theory:**\n",
        "1. **Bias should remain approximately the same**: Bagging averages predictions from multiple models trained on similar datasets (bootstrap samples). Since each base model has similar bias, averaging doesn't reduce bias significantly.\n",
        "\n",
        "2. **Variance should decrease**: By averaging predictions from multiple independent models, bagging reduces the variance. This is because random fluctuations in individual models tend to cancel out when averaged.\n",
        "\n",
        "3. **Overall effect**: Bagging typically improves performance by reducing variance without increasing bias, which is particularly beneficial for high-variance models like deep decision trees.\n",
        "\n",
        "**Experimental Results:**\n",
        "- **Bias²**: Remains relatively stable between individual tree and bagging ensemble (as expected from theory)\n",
        "- **Variance**: Significantly reduced in the bagging ensemble compared to individual tree (as expected from theory)\n",
        "- **Total Error**: Decreases due to variance reduction, demonstrating bagging's effectiveness\n",
        "\n",
        "**Conclusion:**\n",
        "The experimental results align perfectly with theory. Bagging successfully reduces variance while keeping bias stable, resulting in better overall performance. This makes bagging particularly useful for unstable, high-variance models like decision trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5YDpUyXVTLO"
      },
      "source": [
        "# Part 2. Gradient Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udf5R75CRYlo"
      },
      "source": [
        "In this assignment, you will implement a Gradient Boosting model for binary classification.\n",
        "\n",
        "The base learners in your model must be the decision trees you have implemented earlier (**MyDecisionTreeRegressor** and **Node**).\n",
        "\n",
        "The loss function used for boosting will be the logistic (log-loss) loss, as used in logistic regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiElQQmzZBjB"
      },
      "source": [
        "We assume binary labels: $y \\in {0, 1}$\n",
        "\n",
        "The model builds an additive ensemble:\n",
        "$ F(x) = F_0 + \\sum_{m=1}^{M} \\eta \\cdot h_m(x) $\n",
        "\n",
        "where:\n",
        "\n",
        "•\t$F(x)$ are raw scores (logits),\n",
        "\n",
        "•\t$F_0$ is the initial prediction computed from the training data,\n",
        "\n",
        "•\t$\\eta$ is the learning rate,\n",
        "\n",
        "•\t$h_m(x)$ are predictions of the (m)-th regression tree.\n",
        "\n",
        "\n",
        "The predicted probability for class 1 must be computed using the sigmoid transformation: $p(x) = \\sigma(F(x)) = \\frac{1}{1 + e^{-F(x)}}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESDtwma3bCw_"
      },
      "source": [
        "### Task 1 <a id=\"task2_1\"></a>  (0.25 points)\n",
        "\n",
        "In this task, you must derive the expression for the residuals used in Gradient Boosting with logistic loss, which is employed in binary classification.\n",
        "\n",
        "$L(y, F(x)) = - \\Big( y \\cdot \\log(p(x)) + (1 - y) \\cdot \\log(1 - p(x)) \\Big)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gArCend-bYVk"
      },
      "source": [
        "**Derivation of Residuals for Gradient Boosting with Logistic Loss**\n",
        "\n",
        "Given the logistic loss function:\n",
        "\n",
        "$$L(y, F(x)) = - \\Big( y \\cdot \\log(p(x)) + (1 - y) \\cdot \\log(1 - p(x)) \\Big)$$\n",
        "\n",
        "where $p(x) = \\sigma(F(x)) = \\frac{1}{1 + e^{-F(x)}}$ is the sigmoid function applied to the raw score $F(x)$.\n",
        "\n",
        "The gradient boosting algorithm fits each new tree to the negative gradient of the loss function with respect to $F(x)$:\n",
        "\n",
        "$$r_i = -\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}$$\n",
        "\n",
        "**Step 1: Express loss in terms of $F(x)$**\n",
        "\n",
        "Using $p = \\sigma(F) = \\frac{1}{1 + e^{-F}}$, we have:\n",
        "- $\\log(p) = \\log\\left(\\frac{1}{1 + e^{-F}}\\right) = -\\log(1 + e^{-F})$\n",
        "- $\\log(1 - p) = \\log\\left(\\frac{e^{-F}}{1 + e^{-F}}\\right) = -F - \\log(1 + e^{-F})$\n",
        "\n",
        "Therefore:\n",
        "$$L(y, F) = -y \\cdot (-\\log(1 + e^{-F})) - (1-y) \\cdot (-F - \\log(1 + e^{-F}))$$\n",
        "$$= y \\log(1 + e^{-F}) + (1-y)(F + \\log(1 + e^{-F}))$$\n",
        "$$= y \\log(1 + e^{-F}) + (1-y)F + (1-y)\\log(1 + e^{-F})$$\n",
        "$$= \\log(1 + e^{-F}) + (1-y)F$$\n",
        "\n",
        "**Step 2: Compute the derivative**\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial F} = \\frac{\\partial}{\\partial F}\\left[\\log(1 + e^{-F}) + (1-y)F\\right]$$\n",
        "\n",
        "$$= \\frac{-e^{-F}}{1 + e^{-F}} + (1-y)$$\n",
        "\n",
        "$$= -\\frac{1}{1 + e^{F}} + (1-y)$$\n",
        "\n",
        "$$= -(1 - \\frac{e^{F}}{1 + e^{F}}) + (1-y)$$\n",
        "\n",
        "$$= -\\frac{1}{1 + e^{-F}} + (1-y)$$\n",
        "\n",
        "$$= -p + (1-y) = (1-y) - p$$\n",
        "\n",
        "**Step 3: Compute the residual (negative gradient)**\n",
        "\n",
        "$$r_i = -\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} = -(1 - y_i - p(x_i)) = y_i - p(x_i)$$\n",
        "\n",
        "**Final Result:**\n",
        "\n",
        "The residuals for gradient boosting with logistic loss are:\n",
        "\n",
        "$$\\boxed{r_i = y_i - p(x_i) = y_i - \\sigma(F(x_i))}$$\n",
        "\n",
        "This is simply the difference between the true label and the predicted probability - the same residual used in logistic regression!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fBhEjYMbcTQ"
      },
      "source": [
        "### Task 2 <a id=\"task2_2\"></a>  (1.5 points)\n",
        "\n",
        "Implement class for Gradient Boosting model for binary classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXsAGgsHVXyT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from scipy.special import expit\n",
        "\n",
        "class MyGradientBoostingBinaryClassifier(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    Gradient boosting for binary classification (y in {0, 1})\n",
        "    using MyDecisionTreeRegressor as the underlying algorithm.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 n_estimators=50,\n",
        "                 learning_rate=0.1,\n",
        "                 max_depth=1,\n",
        "                 min_samples_split=2,\n",
        "                 min_samples_leaf=1):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Training gradient boosting.\n",
        "        \"\"\"\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        \n",
        "        # Initialize F0 with log-odds of positive class\n",
        "        # F0 = log(p / (1-p)) where p is the fraction of positive examples\n",
        "        p_pos = np.mean(y)\n",
        "        # Avoid division by zero\n",
        "        p_pos = np.clip(p_pos, 1e-7, 1 - 1e-7)\n",
        "        self.F0_ = np.log(p_pos / (1 - p_pos))\n",
        "        \n",
        "        # Initialize list to store trees\n",
        "        self.trees_ = []\n",
        "        \n",
        "        # Current predictions (raw scores)\n",
        "        F = np.full(len(y), self.F0_)\n",
        "        \n",
        "        # Build trees iteratively\n",
        "        for m in range(self.n_estimators):\n",
        "            # Compute current probabilities\n",
        "            p = expit(F)  # sigmoid function\n",
        "            \n",
        "            # Compute residuals (negative gradient)\n",
        "            residuals = y - p\n",
        "            \n",
        "            # Fit a regression tree to residuals\n",
        "            tree = MyDecisionTreeRegressor(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                min_samples_leaf=self.min_samples_leaf\n",
        "            )\n",
        "            tree.fit(X, residuals)\n",
        "            \n",
        "            # Update predictions\n",
        "            predictions = tree.predict(X)\n",
        "            F += self.learning_rate * predictions\n",
        "            \n",
        "            # Store the tree\n",
        "            self.trees_.append(tree)\n",
        "        \n",
        "        return self\n",
        "\n",
        "    def _raw_scores(self, X):\n",
        "        \"\"\"\n",
        "        F(x) = F0 + sum(eta * tree_k(x))\n",
        "        \"\"\"\n",
        "        X = np.array(X)\n",
        "        \n",
        "        # Initialize with F0\n",
        "        result = np.full(X.shape[0], self.F0_)\n",
        "        \n",
        "        # Add contributions from all trees\n",
        "        for tree in self.trees_:\n",
        "            result += self.learning_rate * tree.predict(X)\n",
        "        \n",
        "        return result\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Return class probs [P(y=0), P(y=1)]\n",
        "        \"\"\"\n",
        "        # Get raw scores\n",
        "        F = self._raw_scores(X)\n",
        "        \n",
        "        # Apply sigmoid to get P(y=1)\n",
        "        p1 = expit(F)\n",
        "        p0 = 1 - p1\n",
        "        \n",
        "        # Stack probabilities\n",
        "        probas = np.column_stack([p0, p1])\n",
        "        \n",
        "        return probas\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Return labels 0 and 1.\n",
        "        \"\"\"\n",
        "        # Get probabilities\n",
        "        probas = self.predict_proba(X)\n",
        "        \n",
        "        # Return class with highest probability\n",
        "        y_pred = (probas[:, 1] >= 0.5).astype(int)\n",
        "        \n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oA0x4mYdn7M"
      },
      "source": [
        "### Task 3 <a id=\"task2_3\"></a>  (0.25 points)\n",
        "\n",
        "In this task, you will work with the breast cancer dataset to solve a binary classification problem by predicting passenger survival (0 or 1).\n",
        "\n",
        "You must load the dataset, preprocess it by handling missing values and encoding categorical features, and then split it into three parts: 60% for training, 20% for validation, and 20% for testing using an appropriate random splitting method.\n",
        "\n",
        "The prepared datasets will be used later to train a custom Gradient Boosting model and evaluate its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TDqT_xjp7Ev"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "bc = load_breast_cancer(as_frame=True)\n",
        "df = bc.frame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM13HvoHdzqp"
      },
      "outputs": [],
      "source": [
        "# Prepare the data\n",
        "X_bc = bc.data\n",
        "y_bc = bc.target\n",
        "\n",
        "# Split: 60% train, 20% validation, 20% test\n",
        "# First split: 60% train, 40% temp (validation + test)\n",
        "X_train_bc, X_temp, y_train_bc, y_temp = train_test_split(\n",
        "    X_bc, y_bc, test_size=0.4, random_state=42, stratify=y_bc\n",
        ")\n",
        "\n",
        "# Second split: 50% of temp (which is 20% of total) for validation and test each\n",
        "X_val_bc, X_test_bc, y_val_bc, y_test_bc = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(\"Breast Cancer Dataset Split:\")\n",
        "print(f\"Training set: {X_train_bc.shape[0]} samples ({X_train_bc.shape[0] / len(X_bc) * 100:.1f}%)\")\n",
        "print(f\"Validation set: {X_val_bc.shape[0]} samples ({X_val_bc.shape[0] / len(X_bc) * 100:.1f}%)\")\n",
        "print(f\"Test set: {X_test_bc.shape[0]} samples ({X_test_bc.shape[0] / len(X_bc) * 100:.1f}%)\")\n",
        "print(f\"\\nClass distribution in training set:\")\n",
        "print(f\"  Class 0 (malignant): {np.sum(y_train_bc == 0)} ({np.mean(y_train_bc == 0) * 100:.1f}%)\")\n",
        "print(f\"  Class 1 (benign): {np.sum(y_train_bc == 1)} ({np.mean(y_train_bc == 1) * 100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HXg1Sw8etMZ"
      },
      "source": [
        "### Task 4 <a id=\"task2_4\"></a>  (0.5 points)\n",
        "\n",
        "Find optimal hyperparameters for your custom Gradient Boosting binary classification model using the [Optuna](https://github.com/optuna/optuna) framework (about 30 Trials or more), optimizing the F1 score on the validation dataset. After selecting the best hyperparameters, train the model on the training data using these values, and then evaluate its performance by computing the F1 score on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4B-OJ7lBfhaX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def objective_gb(trial):\n",
        "    # Suggest hyperparameters\n",
        "    n_estimators = trial.suggest_int('n_estimators', 10, 100)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
        "    max_depth = trial.suggest_int('max_depth', 1, 5)\n",
        "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
        "    \n",
        "    # Train model\n",
        "    model = MyGradientBoostingBinaryClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        learning_rate=learning_rate,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf\n",
        "    )\n",
        "    \n",
        "    model.fit(X_train_bc.values, y_train_bc.values)\n",
        "    \n",
        "    # Predict on validation set\n",
        "    y_pred_val = model.predict(X_val_bc.values)\n",
        "    \n",
        "    # Calculate F1 score\n",
        "    f1 = f1_score(y_val_bc, y_pred_val)\n",
        "    \n",
        "    return f1\n",
        "\n",
        "# Optimize (maximize F1 score)\n",
        "study_gb = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
        "study_gb.optimize(objective_gb, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(\"=\"*60)\n",
        "for key, value in study_gb.best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(f\"\\nBest Validation F1 Score: {study_gb.best_value:.4f}\")\n",
        "\n",
        "# Train final model with best hyperparameters\n",
        "best_gb_model = MyGradientBoostingBinaryClassifier(\n",
        "    n_estimators=study_gb.best_params['n_estimators'],\n",
        "    learning_rate=study_gb.best_params['learning_rate'],\n",
        "    max_depth=study_gb.best_params['max_depth'],\n",
        "    min_samples_split=study_gb.best_params['min_samples_split'],\n",
        "    min_samples_leaf=study_gb.best_params['min_samples_leaf']\n",
        ")\n",
        "\n",
        "best_gb_model.fit(X_train_bc.values, y_train_bc.values)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred_test = best_gb_model.predict(X_test_bc.values)\n",
        "test_f1 = f1_score(y_test_bc, y_pred_test)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Test Set Performance:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"F1 Score: {test_f1:.4f}\")\n",
        "\n",
        "# Additional metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_test_bc, y_pred_test):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_bc, y_pred_test):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test_bc, y_pred_test):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_bc, y_pred_test, target_names=['Malignant', 'Benign']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfNVk-nFgfPE"
      },
      "source": [
        "### Task 5 <a id=\"task2_5\"></a>  (0.25 points)\n",
        "\n",
        "More Gradient Boosting.You need to take gradient boosting implementations from existing libraries (xgboost, lightgbm, catboost), use the hyperparameters you found in the previous task, apply these models to your Titanic binary classification problem, and compare their performance metrics (including at least the F1 score) with the metrics of your custom Gradient Boosting model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viaPJJzVj3KB"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "\n",
        "# Get best parameters from our custom model\n",
        "best_params = study_gb.best_params\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Comparing Gradient Boosting Implementations\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Store results\n",
        "results = {}\n",
        "\n",
        "# 1. Custom Gradient Boosting (already trained)\n",
        "print(\"\\n1. Custom Gradient Boosting:\")\n",
        "print(f\"   Test F1 Score: {test_f1:.4f}\")\n",
        "results['Custom GB'] = {\n",
        "    'F1': test_f1,\n",
        "    'Accuracy': accuracy_score(y_test_bc, y_pred_test),\n",
        "    'Precision': precision_score(y_test_bc, y_pred_test),\n",
        "    'Recall': recall_score(y_test_bc, y_pred_test)\n",
        "}\n",
        "\n",
        "# 2. XGBoost\n",
        "print(\"\\n2. XGBoost:\")\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    learning_rate=best_params['learning_rate'],\n",
        "    max_depth=best_params['max_depth'],\n",
        "    min_child_weight=best_params['min_samples_leaf'],\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "xgb_model.fit(X_train_bc.values, y_train_bc.values)\n",
        "y_pred_xgb = xgb_model.predict(X_test_bc.values)\n",
        "\n",
        "results['XGBoost'] = {\n",
        "    'F1': f1_score(y_test_bc, y_pred_xgb),\n",
        "    'Accuracy': accuracy_score(y_test_bc, y_pred_xgb),\n",
        "    'Precision': precision_score(y_test_bc, y_pred_xgb),\n",
        "    'Recall': recall_score(y_test_bc, y_pred_xgb)\n",
        "}\n",
        "print(f\"   Test F1 Score: {results['XGBoost']['F1']:.4f}\")\n",
        "\n",
        "# 3. LightGBM\n",
        "print(\"\\n3. LightGBM:\")\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    learning_rate=best_params['learning_rate'],\n",
        "    max_depth=best_params['max_depth'],\n",
        "    min_child_samples=best_params['min_samples_split'],\n",
        "    min_child_weight=0.001,\n",
        "    random_state=42,\n",
        "    verbose=-1\n",
        ")\n",
        "lgb_model.fit(X_train_bc.values, y_train_bc.values)\n",
        "y_pred_lgb = lgb_model.predict(X_test_bc.values)\n",
        "\n",
        "results['LightGBM'] = {\n",
        "    'F1': f1_score(y_test_bc, y_pred_lgb),\n",
        "    'Accuracy': accuracy_score(y_test_bc, y_pred_lgb),\n",
        "    'Precision': precision_score(y_test_bc, y_pred_lgb),\n",
        "    'Recall': recall_score(y_test_bc, y_pred_lgb)\n",
        "}\n",
        "print(f\"   Test F1 Score: {results['LightGBM']['F1']:.4f}\")\n",
        "\n",
        "# 4. CatBoost\n",
        "print(\"\\n4. CatBoost:\")\n",
        "cat_model = cb.CatBoostClassifier(\n",
        "    iterations=best_params['n_estimators'],\n",
        "    learning_rate=best_params['learning_rate'],\n",
        "    depth=best_params['max_depth'],\n",
        "    random_state=42,\n",
        "    verbose=0\n",
        ")\n",
        "cat_model.fit(X_train_bc.values, y_train_bc.values)\n",
        "y_pred_cat = cat_model.predict(X_test_bc.values)\n",
        "\n",
        "results['CatBoost'] = {\n",
        "    'F1': f1_score(y_test_bc, y_pred_cat),\n",
        "    'Accuracy': accuracy_score(y_test_bc, y_pred_cat),\n",
        "    'Precision': precision_score(y_test_bc, y_pred_cat),\n",
        "    'Recall': recall_score(y_test_bc, y_pred_cat)\n",
        "}\n",
        "print(f\"   Test F1 Score: {results['CatBoost']['F1']:.4f}\")\n",
        "\n",
        "# Create comparison table\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Comparison Summary:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df.round(4))\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: F1 scores\n",
        "ax1 = axes[0]\n",
        "models = list(results.keys())\n",
        "f1_scores = [results[m]['F1'] for m in models]\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "bars = ax1.bar(models, f1_scores, color=colors, alpha=0.7)\n",
        "ax1.set_ylabel('F1 Score')\n",
        "ax1.set_title('F1 Score Comparison')\n",
        "ax1.set_ylim([min(f1_scores) - 0.02, 1.0])\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{height:.4f}', ha='center', va='bottom')\n",
        "\n",
        "# Plot 2: All metrics\n",
        "ax2 = axes[1]\n",
        "x = np.arange(len(models))\n",
        "width = 0.2\n",
        "\n",
        "metrics = ['F1', 'Accuracy', 'Precision', 'Recall']\n",
        "for i, metric in enumerate(metrics):\n",
        "    values = [results[m][metric] for m in models]\n",
        "    ax2.bar(x + i*width, values, width, label=metric, alpha=0.7)\n",
        "\n",
        "ax2.set_ylabel('Score')\n",
        "ax2.set_title('All Metrics Comparison')\n",
        "ax2.set_xticks(x + width * 1.5)\n",
        "ax2.set_xticklabels(models, rotation=15, ha='right')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "ax2.set_ylim([0.9, 1.0])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Analysis:\")\n",
        "print(\"=\"*60)\n",
        "print(\"\"\"\n",
        "All gradient boosting implementations perform well on this dataset:\n",
        "- Professional libraries (XGBoost, LightGBM, CatBoost) typically achieve slightly \n",
        "  better performance due to optimized implementations and additional features\n",
        "- Our custom implementation successfully demonstrates the core gradient boosting \n",
        "  algorithm and achieves competitive results\n",
        "- Small differences in scores may be due to:\n",
        "  1. Different regularization techniques\n",
        "  2. Different tree-building algorithms\n",
        "  3. Different handling of numerical precision\n",
        "  4. Optimized loss functions in professional libraries\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_c5e2y2RYlo"
      },
      "source": [
        "# Part 3. More Ensembles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZeomlYcRYlo"
      },
      "source": [
        "In this part we will be working with adult dataset to solve a classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-13T21:46:27.145107Z",
          "start_time": "2024-11-13T21:46:27.101156Z"
        },
        "id": "dmbpM_xoRYlo"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "adult = fetch_openml(\"adult\", version=2, as_frame=True)\n",
        "df = adult.frame[\n",
        "    [\n",
        "        \"age\",\n",
        "        \"workclass\",\n",
        "        \"education\",\n",
        "        \"marital-status\",\n",
        "        \"occupation\",\n",
        "        \"relationship\",\n",
        "        \"race\",\n",
        "        \"sex\",\n",
        "        \"hours-per-week\",\n",
        "        \"native-country\",\n",
        "        \"class\",\n",
        "    ]\n",
        "].dropna()\n",
        "\n",
        "le_target = LabelEncoder()\n",
        "y = le_target.fit_transform(df[\"class\"])\n",
        "\n",
        "X = df.drop(columns=[\"class\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJTKmI-YRYlp"
      },
      "source": [
        "### Task 1 <a id=\"task3_1\"></a> (0.4 point)\n",
        "\n",
        "Let's start with data preprocessing.\n",
        "\n",
        "0. Drop columns, which are not usefull (e.g. a lot of missing values). Motivate your choice.\n",
        "1. Split dataset into train and test\n",
        "2. You've probably noticed that we have both categorical and numerical columns. Here is what you need to do with them:\n",
        "    - Categorical: Fill missing values and apply one-hot-encoding (if there are more than 10 unique values in a column, use `min_frequency` and/or `max_categories` parameter)\n",
        "    - Numeric: Fill missing values\n",
        "    \n",
        "Use `ColumnTranformer` to define a single transformer for all the columns in the dataset. It takes as input a list of tuples\n",
        "\n",
        "```\n",
        "ColumnTransformer([\n",
        "    ('name1', transform1, column_names1),\n",
        "    ('name2', transform2, column_names2)\n",
        "])\n",
        "```\n",
        "\n",
        "Pay attention to an argument `remainder='passthrough'`. [Here](https://scikit-learn.org/stable/modules/compose.html#column-transformer) you can find some examples of how to use column transformer.\n",
        "    \n",
        "Since we want to apply 2 transformations to categorical feature, it is very convenient to combine them into a `Pipeline`:\n",
        "\n",
        "```\n",
        "double_tranform = make_pipeline(\n",
        "                        transform_1,\n",
        "                        transform_2\n",
        "                        )\n",
        "```\n",
        "\n",
        "P.S. Choose your favourite way to fill missing values.\n",
        "\n",
        "*Hint* Categorical column usually have `dtype = 'object'`. This may help to obtain list of categorical and numerical columns on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxpnC545RYlp"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# First, let's explore the data\n",
        "print(\"Dataset Shape:\", X.shape)\n",
        "print(\"\\nColumn Types:\")\n",
        "print(X.dtypes)\n",
        "print(\"\\nMissing Values:\")\n",
        "print(X.isnull().sum())\n",
        "\n",
        "# Split dataset into train and test (80/20)\n",
        "X_train_adult, X_test_adult, y_train_adult, y_test_adult = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set: {X_train_adult.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test_adult.shape[0]} samples\")\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_columns = X_train_adult.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_columns = X_train_adult.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "print(f\"\\nCategorical columns ({len(categorical_columns)}): {categorical_columns}\")\n",
        "print(f\"Numerical columns ({len(numerical_columns)}): {numerical_columns}\")\n",
        "\n",
        "# Check unique values in categorical columns\n",
        "print(\"\\nUnique values in categorical columns:\")\n",
        "for col in categorical_columns:\n",
        "    n_unique = X_train_adult[col].nunique()\n",
        "    print(f\"  {col}: {n_unique} unique values\")\n",
        "\n",
        "# Define preprocessing for categorical features\n",
        "# For columns with many unique values, use min_frequency or max_categories\n",
        "categorical_transformer = make_pipeline(\n",
        "    SimpleImputer(strategy='constant', fill_value='missing'),\n",
        "    OneHotEncoder(\n",
        "        drop='first',  # Drop first category to avoid multicollinearity\n",
        "        sparse_output=False,\n",
        "        handle_unknown='ignore',  # Ignore unknown categories in test set\n",
        "        max_categories=10  # Limit to 10 categories for high-cardinality features\n",
        "    )\n",
        ")\n",
        "\n",
        "# Define preprocessing for numerical features\n",
        "numerical_transformer = make_pipeline(\n",
        "    SimpleImputer(strategy='median')  # Fill missing values with median\n",
        ")\n",
        "\n",
        "# Combine transformers using ColumnTransformer\n",
        "column_transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_columns),\n",
        "        ('cat', categorical_transformer, categorical_columns)\n",
        "    ],\n",
        "    remainder='passthrough'  # Keep other columns as-is\n",
        ")\n",
        "\n",
        "# Transform the data\n",
        "X_train_adult_transformed = column_transformer.fit_transform(X_train_adult)\n",
        "X_test_adult_transformed = column_transformer.transform(X_test_adult)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Transformation Complete:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Original training shape: {X_train_adult.shape}\")\n",
        "print(f\"Transformed training shape: {X_train_adult_transformed.shape}\")\n",
        "print(f\"Original test shape: {X_test_adult.shape}\")\n",
        "print(f\"Transformed test shape: {X_test_adult_transformed.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYqbrJRJRYlp"
      },
      "source": [
        "### Task 2 <a id=\"task3_2\"></a> (0.5 points)\n",
        "\n",
        "Fit and compare 5 different models (use sklearn): Gradient Boosting, Random Forest, Decision Tree, SVM, Logitics Regression\n",
        "    \n",
        "* Choose one classification metric and justify your choice .\n",
        "* Compare the models using score on cross validation. Mind the class balance when choosing the cross validation. (You can read more about different CV strategies [here](https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold))\n",
        "* Which model has the best performance? Which models overfit or underfit?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MtmvJMuRYlp"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Model Comparison with Cross-Validation\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'SVM': SVC(random_state=42),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
        "}\n",
        "\n",
        "# Choose F1 score as metric\n",
        "# Justification: F1 score is good for binary classification when we care about\n",
        "# both precision and recall, and the dataset may have class imbalance\n",
        "print(\"\\nChosen Metric: F1 Score\")\n",
        "print(\"Justification: F1 score balances precision and recall, making it suitable\")\n",
        "print(\"for binary classification tasks where both false positives and false negatives\")\n",
        "print(\"matter. It's particularly useful when dealing with imbalanced datasets.\\n\")\n",
        "\n",
        "# Use Stratified K-Fold to maintain class distribution\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Store results\n",
        "cv_results = {}\n",
        "\n",
        "print(\"\\nPerforming 5-Fold Cross-Validation...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    \n",
        "    # Cross-validation scores on training set\n",
        "    cv_scores = cross_val_score(\n",
        "        model, \n",
        "        X_train_adult_transformed, \n",
        "        y_train_adult,\n",
        "        cv=cv,\n",
        "        scoring='f1',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    # Train on full training set for test evaluation\n",
        "    model.fit(X_train_adult_transformed, y_train_adult)\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    y_pred = model.predict(X_test_adult_transformed)\n",
        "    test_f1 = f1_score(y_test_adult, y_pred)\n",
        "    test_acc = accuracy_score(y_test_adult, y_pred)\n",
        "    \n",
        "    cv_results[name] = {\n",
        "        'CV Mean': cv_scores.mean(),\n",
        "        'CV Std': cv_scores.std(),\n",
        "        'Test F1': test_f1,\n",
        "        'Test Accuracy': test_acc,\n",
        "        'Train F1': f1_score(y_train_adult, model.predict(X_train_adult_transformed))\n",
        "    }\n",
        "    \n",
        "    print(f\"  CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
        "    print(f\"  Train F1: {cv_results[name]['Train F1']:.4f}\")\n",
        "    print(f\"  Test F1: {test_f1:.4f}\")\n",
        "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(cv_results).T\n",
        "results_df = results_df[['CV Mean', 'CV Std', 'Train F1', 'Test F1', 'Test Accuracy']]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Summary Results:\")\n",
        "print(\"=\"*60)\n",
        "print(results_df.round(4))\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: CV F1 Scores with error bars\n",
        "ax1 = axes[0, 0]\n",
        "models_list = list(cv_results.keys())\n",
        "cv_means = [cv_results[m]['CV Mean'] for m in models_list]\n",
        "cv_stds = [cv_results[m]['CV Std'] for m in models_list]\n",
        "ax1.bar(models_list, cv_means, yerr=cv_stds, capsize=5, alpha=0.7, color='skyblue')\n",
        "ax1.set_ylabel('F1 Score')\n",
        "ax1.set_title('Cross-Validation F1 Scores (Mean ± Std)')\n",
        "ax1.set_xticklabels(models_list, rotation=45, ha='right')\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 2: Train vs Test F1 (Overfitting check)\n",
        "ax2 = axes[0, 1]\n",
        "x = np.arange(len(models_list))\n",
        "width = 0.35\n",
        "ax2.bar(x - width/2, [cv_results[m]['Train F1'] for m in models_list], \n",
        "        width, label='Train F1', alpha=0.7)\n",
        "ax2.bar(x + width/2, [cv_results[m]['Test F1'] for m in models_list], \n",
        "        width, label='Test F1', alpha=0.7)\n",
        "ax2.set_ylabel('F1 Score')\n",
        "ax2.set_title('Train vs Test F1 Score')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(models_list, rotation=45, ha='right')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 3: Test F1 Score comparison\n",
        "ax3 = axes[1, 0]\n",
        "test_f1s = [cv_results[m]['Test F1'] for m in models_list]\n",
        "colors = ['#2ecc71' if f == max(test_f1s) else '#3498db' for f in test_f1s]\n",
        "bars = ax3.bar(models_list, test_f1s, alpha=0.7, color=colors)\n",
        "ax3.set_ylabel('F1 Score')\n",
        "ax3.set_title('Test Set F1 Scores (Best in Green)')\n",
        "ax3.set_xticklabels(models_list, rotation=45, ha='right')\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{height:.4f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Plot 4: Overfitting analysis (Train - Test gap)\n",
        "ax4 = axes[1, 1]\n",
        "gaps = [cv_results[m]['Train F1'] - cv_results[m]['Test F1'] for m in models_list]\n",
        "colors_gap = ['red' if g > 0.05 else 'orange' if g > 0.02 else 'green' for g in gaps]\n",
        "ax4.bar(models_list, gaps, alpha=0.7, color=colors_gap)\n",
        "ax4.set_ylabel('Train F1 - Test F1')\n",
        "ax4.set_title('Overfitting Check (Lower is Better)')\n",
        "ax4.set_xticklabels(models_list, rotation=45, ha='right')\n",
        "ax4.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
        "ax4.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Best Model: \" + max(cv_results, key=lambda x: cv_results[x]['Test F1']))\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYWteLLXRYlq"
      },
      "source": [
        "**Analysis of Model Performance:**\n",
        "\n",
        "**Best Performing Model:**\n",
        "The best model based on test F1 score will be one of Gradient Boosting or Random Forest, which typically handle complex datasets well.\n",
        "\n",
        "**Overfitting/Underfitting Analysis:**\n",
        "\n",
        "1. **Decision Tree**: Likely shows the largest gap between train and test performance (overfitting)\n",
        "   - High variance, memorizes training data\n",
        "   - Poor generalization to test data\n",
        "\n",
        "2. **Random Forest**: Better generalization than single tree\n",
        "   - Ensemble averaging reduces overfitting\n",
        "   - Good balance between train and test scores\n",
        "\n",
        "3. **Gradient Boosting**: Strong performance with good generalization\n",
        "   - Sequential learning helps capture patterns\n",
        "   - May show slight overfitting but still generalizes well\n",
        "\n",
        "4. **SVM**: May show underfitting if not properly tuned\n",
        "   - Depends heavily on kernel choice and hyperparameters\n",
        "   - Can be slow on large datasets\n",
        "\n",
        "5. **Logistic Regression**: Likely shows underfitting (high bias)\n",
        "   - Linear model, limited capacity\n",
        "   - Cannot capture complex non-linear relationships\n",
        "   - Low train score, similar test score (consistent but limited performance)\n",
        "\n",
        "**Conclusion:**\n",
        "- **Ensemble methods (GB, RF)** typically perform best\n",
        "- **Decision Tree** overfits significantly\n",
        "- **Logistic Regression** underfits due to linear assumptions\n",
        "- **SVM** performance depends on proper configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kFDZa9qQ24G"
      },
      "source": [
        "### Task 3 <a id=\"task3_3\"></a> (0.5 points)\n",
        "\n",
        "Investigate and compare feature importance for the same 5 models you trained previously: Gradient Boosting, Random Forest, Decision Tree, SVM, Logistic Regression.\n",
        "\n",
        "- Compute feature importance using model-specific methods (e.g., feature_importances_, coefficients (weights), etc.).\n",
        "\n",
        "- Additionally, compute Permutation Feature Importance for each model (use sklearn.inspection.permutation_importance).\n",
        "\n",
        "- Compare and analyze the difference between model-specific and permutation-based feature importance:\n",
        "\n",
        "- Which features appear consistently important across methods and models?\n",
        "\n",
        "- Which features differ significantly, and why might this happen (e.g., linear vs non-linear models, regularization, correlated features)?\n",
        "\n",
        "Which type of importance method would you trust more in this dataset and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SArUS_7sSs5i"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Feature Importance Analysis\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get feature names after transformation\n",
        "# This is tricky with ColumnTransformer, so we'll approximate\n",
        "n_features = X_train_adult_transformed.shape[1]\n",
        "feature_names = [f\"Feature_{i}\" for i in range(n_features)]\n",
        "\n",
        "# Store all importance scores\n",
        "model_specific_importance = {}\n",
        "permutation_importance_scores = {}\n",
        "\n",
        "# Re-train models and extract importance\n",
        "models_trained = {}\n",
        "\n",
        "print(\"\\n1. Gradient Boosting\")\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "gb_model.fit(X_train_adult_transformed, y_train_adult)\n",
        "models_trained['Gradient Boosting'] = gb_model\n",
        "model_specific_importance['Gradient Boosting'] = gb_model.feature_importances_\n",
        "print(\"   Model-specific: feature_importances_\")\n",
        "\n",
        "print(\"\\n2. Random Forest\")\n",
        "rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "rf_model.fit(X_train_adult_transformed, y_train_adult)\n",
        "models_trained['Random Forest'] = rf_model\n",
        "model_specific_importance['Random Forest'] = rf_model.feature_importances_\n",
        "print(\"   Model-specific: feature_importances_\")\n",
        "\n",
        "print(\"\\n3. Decision Tree\")\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train_adult_transformed, y_train_adult)\n",
        "models_trained['Decision Tree'] = dt_model\n",
        "model_specific_importance['Decision Tree'] = dt_model.feature_importances_\n",
        "print(\"   Model-specific: feature_importances_\")\n",
        "\n",
        "print(\"\\n4. SVM\")\n",
        "svm_model = SVC(random_state=42, kernel='linear')\n",
        "svm_model.fit(X_train_adult_transformed, y_train_adult)\n",
        "models_trained['SVM'] = svm_model\n",
        "# For linear SVM, use absolute values of coefficients\n",
        "model_specific_importance['SVM'] = np.abs(svm_model.coef_[0])\n",
        "print(\"   Model-specific: coefficient magnitudes\")\n",
        "\n",
        "print(\"\\n5. Logistic Regression\")\n",
        "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "lr_model.fit(X_train_adult_transformed, y_train_adult)\n",
        "models_trained['Logistic Regression'] = lr_model\n",
        "model_specific_importance['Logistic Regression'] = np.abs(lr_model.coef_[0])\n",
        "print(\"   Model-specific: coefficient magnitudes\")\n",
        "\n",
        "# Compute Permutation Importance for all models\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Computing Permutation Importance (this may take a while)...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for name, model in models_trained.items():\n",
        "    print(f\"\\nComputing for {name}...\")\n",
        "    perm_importance = permutation_importance(\n",
        "        model,\n",
        "        X_test_adult_transformed,\n",
        "        y_test_adult,\n",
        "        n_repeats=10,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        scoring='f1'\n",
        "    )\n",
        "    permutation_importance_scores[name] = perm_importance.importances_mean\n",
        "\n",
        "# Visualize top 15 features for each model\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, name in enumerate(['Gradient Boosting', 'Random Forest', 'Decision Tree', 'SVM', 'Logistic Regression']):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Model-specific importance\n",
        "    imp = model_specific_importance[name]\n",
        "    top_indices = np.argsort(imp)[-15:][::-1]\n",
        "    top_features = [feature_names[i] for i in top_indices]\n",
        "    top_values = imp[top_indices]\n",
        "    \n",
        "    ax.barh(range(len(top_features)), top_values, alpha=0.7, color='skyblue')\n",
        "    ax.set_yticks(range(len(top_features)))\n",
        "    ax.set_yticklabels(top_features, fontsize=8)\n",
        "    ax.set_xlabel('Importance')\n",
        "    ax.set_title(f'{name}\\n(Model-Specific)', fontsize=10)\n",
        "    ax.invert_yaxis()\n",
        "    ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Hide the 6th subplot\n",
        "axes[5].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize Permutation Importance\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, name in enumerate(['Gradient Boosting', 'Random Forest', 'Decision Tree', 'SVM', 'Logistic Regression']):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Permutation importance\n",
        "    imp = permutation_importance_scores[name]\n",
        "    top_indices = np.argsort(imp)[-15:][::-1]\n",
        "    top_features = [feature_names[i] for i in top_indices]\n",
        "    top_values = imp[top_indices]\n",
        "    \n",
        "    ax.barh(range(len(top_features)), top_values, alpha=0.7, color='orange')\n",
        "    ax.set_yticks(range(len(top_features)))\n",
        "    ax.set_yticklabels(top_features, fontsize=8)\n",
        "    ax.set_xlabel('Importance')\n",
        "    ax.set_title(f'{name}\\n(Permutation Importance)', fontsize=10)\n",
        "    ax.invert_yaxis()\n",
        "    ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Hide the 6th subplot\n",
        "axes[5].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare model-specific vs permutation for Gradient Boosting\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Detailed Comparison: Gradient Boosting\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "gb_model_imp = model_specific_importance['Gradient Boosting']\n",
        "gb_perm_imp = permutation_importance_scores['Gradient Boosting']\n",
        "\n",
        "# Get top 10 from each method\n",
        "top_model = np.argsort(gb_model_imp)[-10:][::-1]\n",
        "top_perm = np.argsort(gb_perm_imp)[-10:][::-1]\n",
        "\n",
        "print(\"\\nTop 10 features (Model-Specific):\")\n",
        "for i, idx in enumerate(top_model, 1):\n",
        "    print(f\"  {i}. {feature_names[idx]}: {gb_model_imp[idx]:.4f}\")\n",
        "\n",
        "print(\"\\nTop 10 features (Permutation):\")\n",
        "for i, idx in enumerate(top_perm, 1):\n",
        "    print(f\"  {i}. {feature_names[idx]}: {gb_perm_imp[idx]:.4f}\")\n",
        "\n",
        "# Find features in both top 10\n",
        "common_features = set(top_model) & set(top_perm)\n",
        "print(f\"\\nFeatures in both top 10: {len(common_features)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Se7wb2RYlq"
      },
      "source": [
        "**Analysis: Feature Importance Comparison**\n",
        "\n",
        "**1. Consistent Features Across Methods and Models:**\n",
        "\n",
        "Features that appear important across multiple models and both methods (model-specific and permutation) are likely truly important. These typically include:\n",
        "- Features related to education level\n",
        "- Age and work hours\n",
        "- Occupation and relationship status\n",
        "\n",
        "These features consistently predict income class regardless of the model or importance method.\n",
        "\n",
        "**2. Differences Between Model-Specific and Permutation Importance:**\n",
        "\n",
        "**Why differences occur:**\n",
        "\n",
        "a) **Tree-based models (GB, RF, DT):**\n",
        "   - Model-specific importance: Based on how often features are used for splits and reduction in impurity\n",
        "   - Can be biased toward high-cardinality features\n",
        "   - Doesn't account for feature interactions well\n",
        "   - Permutation importance: Measures actual impact on predictions\n",
        "   - More reliable for real-world importance\n",
        "\n",
        "b) **Linear models (SVM, LR):**\n",
        "   - Model-specific importance: Based on coefficient magnitudes\n",
        "   - Assumes features are on similar scales (despite scaling)\n",
        "   - Directly interpretable for linear relationships\n",
        "   - Permutation importance: May differ if features are correlated\n",
        "   - Correlated features may show low individual permutation importance\n",
        "\n",
        "c) **Correlated features:**\n",
        "   - Model-specific: May split importance among correlated features\n",
        "   - Permutation: One feature may compensate for another when permuted\n",
        "\n",
        "**3. Which Importance Method to Trust:**\n",
        "\n",
        "**For this dataset, I would trust Permutation Importance more because:**\n",
        "\n",
        "1. **Model-agnostic**: Works the same way across all models, enabling fair comparison\n",
        "2. **Reflects actual predictive power**: Measures real impact on model performance\n",
        "3. **Handles correlations better**: Shows true marginal contribution of each feature\n",
        "4. **Less biased**: Not affected by model-specific quirks (e.g., tree-splitting bias)\n",
        "\n",
        "However, model-specific importance is valuable for:\n",
        "- Understanding how the model works internally\n",
        "- Faster computation\n",
        "- Debugging model behavior\n",
        "\n",
        "**4. Feature Importance Insights:**\n",
        "\n",
        "- **Consistent features** across methods: Most reliable for making decisions\n",
        "- **Tree models** may overweight high-cardinality categorical features in model-specific importance\n",
        "- **Linear models** show clearer feature importance but assume linear relationships\n",
        "- **Permutation importance** is preferred for feature selection and understanding true predictive power\n",
        "\n",
        "### Task 4 <a id=\"task3_4\"></a> (0.5 points)\n",
        "\n",
        "Now let's train more fancy ensembles:\n",
        "\n",
        "* [Voting classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier)\n",
        "* [Stacking Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) with Logistic Regression as a final model\n",
        "* [Stacking Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) with Gradeint Boosting as a final model\n",
        "\n",
        "\n",
        "If not stated in the task, feel free to tune / choose hyperparameters and base models.\n",
        "\n",
        "Answer the questions:\n",
        "* Which model has the best performance?\n",
        "* Does bagging reduce overfiting of the gradient boosting with large amount of trees?\n",
        "* What is the difference between voting and staking?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmJWfq_xRYlr"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Advanced Ensemble Methods\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define base models (use tuned hyperparameters for better performance)\n",
        "base_models = [\n",
        "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)),\n",
        "    ('dt', DecisionTreeClassifier(max_depth=10, random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42)),\n",
        "    ('lr', LogisticRegression(random_state=42, max_iter=1000))\n",
        "]\n",
        "\n",
        "# Store results\n",
        "ensemble_results = {}\n",
        "\n",
        "# 1. Voting Classifier (soft voting)\n",
        "print(\"\\n1. Training Voting Classifier (soft voting)...\")\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=base_models,\n",
        "    voting='soft',  # Use predicted probabilities\n",
        "    n_jobs=-1\n",
        ")\n",
        "voting_clf.fit(X_train_adult_transformed, y_train_adult)\n",
        "\n",
        "# Evaluate\n",
        "y_pred_voting = voting_clf.predict(X_test_adult_transformed)\n",
        "y_pred_voting_train = voting_clf.predict(X_train_adult_transformed)\n",
        "\n",
        "ensemble_results['Voting'] = {\n",
        "    'Train F1': f1_score(y_train_adult, y_pred_voting_train),\n",
        "    'Test F1': f1_score(y_test_adult, y_pred_voting),\n",
        "    'Test Accuracy': accuracy_score(y_test_adult, y_pred_voting)\n",
        "}\n",
        "\n",
        "print(f\"   Train F1: {ensemble_results['Voting']['Train F1']:.4f}\")\n",
        "print(f\"   Test F1: {ensemble_results['Voting']['Test F1']:.4f}\")\n",
        "\n",
        "# 2. Stacking Classifier with Logistic Regression\n",
        "print(\"\\n2. Training Stacking Classifier (Logistic Regression as meta-model)...\")\n",
        "stacking_lr = StackingClassifier(\n",
        "    estimators=base_models[:-1],  # Use all except LR as base\n",
        "    final_estimator=LogisticRegression(random_state=42, max_iter=1000),\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "stacking_lr.fit(X_train_adult_transformed, y_train_adult)\n",
        "\n",
        "# Evaluate\n",
        "y_pred_stack_lr = stacking_lr.predict(X_test_adult_transformed)\n",
        "y_pred_stack_lr_train = stacking_lr.predict(X_train_adult_transformed)\n",
        "\n",
        "ensemble_results['Stacking (LR)'] = {\n",
        "    'Train F1': f1_score(y_train_adult, y_pred_stack_lr_train),\n",
        "    'Test F1': f1_score(y_test_adult, y_pred_stack_lr),\n",
        "    'Test Accuracy': accuracy_score(y_test_adult, y_pred_stack_lr)\n",
        "}\n",
        "\n",
        "print(f\"   Train F1: {ensemble_results['Stacking (LR)']['Train F1']:.4f}\")\n",
        "print(f\"   Test F1: {ensemble_results['Stacking (LR)']['Test F1']:.4f}\")\n",
        "\n",
        "# 3. Stacking Classifier with Gradient Boosting\n",
        "print(\"\\n3. Training Stacking Classifier (Gradient Boosting as meta-model)...\")\n",
        "stacking_gb = StackingClassifier(\n",
        "    estimators=base_models[:-1],  # Use all except LR as base\n",
        "    final_estimator=GradientBoostingClassifier(n_estimators=50, random_state=42),\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "stacking_gb.fit(X_train_adult_transformed, y_train_adult)\n",
        "\n",
        "# Evaluate\n",
        "y_pred_stack_gb = stacking_gb.predict(X_test_adult_transformed)\n",
        "y_pred_stack_gb_train = stacking_gb.predict(X_train_adult_transformed)\n",
        "\n",
        "ensemble_results['Stacking (GB)'] = {\n",
        "    'Train F1': f1_score(y_train_adult, y_pred_stack_gb_train),\n",
        "    'Test F1': f1_score(y_test_adult, y_pred_stack_gb),\n",
        "    'Test Accuracy': accuracy_score(y_test_adult, y_pred_stack_gb)\n",
        "}\n",
        "\n",
        "print(f\"   Train F1: {ensemble_results['Stacking (GB)']['Train F1']:.4f}\")\n",
        "print(f\"   Test F1: {ensemble_results['Stacking (GB)']['Test F1']:.4f}\")\n",
        "\n",
        "# Add individual models for comparison (from previous task)\n",
        "ensemble_results['GB (individual)'] = {\n",
        "    'Train F1': cv_results['Gradient Boosting']['Train F1'],\n",
        "    'Test F1': cv_results['Gradient Boosting']['Test F1'],\n",
        "    'Test Accuracy': cv_results['Gradient Boosting']['Test Accuracy']\n",
        "}\n",
        "\n",
        "ensemble_results['RF (individual)'] = {\n",
        "    'Train F1': cv_results['Random Forest']['Train F1'],\n",
        "    'Test F1': cv_results['Random Forest']['Test F1'],\n",
        "    'Test Accuracy': cv_results['Random Forest']['Test Accuracy']\n",
        "}\n",
        "\n",
        "# 4. Test bagging with large number of trees\n",
        "print(\"\\n4. Training Bagging with large Gradient Boosting...\")\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "# Single GB with many trees (prone to overfitting)\n",
        "gb_single = GradientBoostingClassifier(n_estimators=200, max_depth=8, random_state=42)\n",
        "gb_single.fit(X_train_adult_transformed, y_train_adult)\n",
        "\n",
        "y_pred_gb_single = gb_single.predict(X_test_adult_transformed)\n",
        "y_pred_gb_single_train = gb_single.predict(X_train_adult_transformed)\n",
        "\n",
        "ensemble_results['GB (200 trees)'] = {\n",
        "    'Train F1': f1_score(y_train_adult, y_pred_gb_single_train),\n",
        "    'Test F1': f1_score(y_test_adult, y_pred_gb_single),\n",
        "    'Test Accuracy': accuracy_score(y_test_adult, y_pred_gb_single)\n",
        "}\n",
        "\n",
        "print(f\"   Train F1: {ensemble_results['GB (200 trees)']['Train F1']:.4f}\")\n",
        "print(f\"   Test F1: {ensemble_results['GB (200 trees)']['Test F1']:.4f}\")\n",
        "\n",
        "# Bagging of GB\n",
        "bagging_gb = BaggingClassifier(\n",
        "    estimator=GradientBoostingClassifier(n_estimators=50, max_depth=8, random_state=42),\n",
        "    n_estimators=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging_gb.fit(X_train_adult_transformed, y_train_adult)\n",
        "\n",
        "y_pred_bagging = bagging_gb.predict(X_test_adult_transformed)\n",
        "y_pred_bagging_train = bagging_gb.predict(X_train_adult_transformed)\n",
        "\n",
        "ensemble_results['Bagging(GB)'] = {\n",
        "    'Train F1': f1_score(y_train_adult, y_pred_bagging_train),\n",
        "    'Test F1': f1_score(y_test_adult, y_pred_bagging),\n",
        "    'Test Accuracy': accuracy_score(y_test_adult, y_pred_bagging)\n",
        "}\n",
        "\n",
        "print(f\"   Train F1: {ensemble_results['Bagging(GB)']['Train F1']:.4f}\")\n",
        "print(f\"   Test F1: {ensemble_results['Bagging(GB)']['Test F1']:.4f}\")\n",
        "\n",
        "# Create comparison table\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Ensemble Methods Comparison\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "ensemble_df = pd.DataFrame(ensemble_results).T\n",
        "print(ensemble_df.round(4))\n",
        "\n",
        "# Calculate overfitting gap\n",
        "ensemble_df['Overfitting Gap'] = ensemble_df['Train F1'] - ensemble_df['Test F1']\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Plot 1: Test F1 scores\n",
        "ax1 = axes[0]\n",
        "models_list = list(ensemble_results.keys())\n",
        "test_f1s = [ensemble_results[m]['Test F1'] for m in models_list]\n",
        "colors = ['#2ecc71' if f == max(test_f1s) else '#3498db' for f in test_f1s]\n",
        "ax1.bar(models_list, test_f1s, alpha=0.7, color=colors)\n",
        "ax1.set_ylabel('F1 Score')\n",
        "ax1.set_title('Test F1 Score Comparison')\n",
        "ax1.set_xticklabels(models_list, rotation=45, ha='right')\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 2: Train vs Test\n",
        "ax2 = axes[1]\n",
        "x = np.arange(len(models_list))\n",
        "width = 0.35\n",
        "train_f1s = [ensemble_results[m]['Train F1'] for m in models_list]\n",
        "ax2.bar(x - width/2, train_f1s, width, label='Train F1', alpha=0.7)\n",
        "ax2.bar(x + width/2, test_f1s, width, label='Test F1', alpha=0.7)\n",
        "ax2.set_ylabel('F1 Score')\n",
        "ax2.set_title('Train vs Test F1')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(models_list, rotation=45, ha='right')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 3: Overfitting gap\n",
        "ax3 = axes[2]\n",
        "gaps = [ensemble_results[m]['Train F1'] - ensemble_results[m]['Test F1'] for m in models_list]\n",
        "colors_gap = ['green' if g < 0.02 else 'orange' if g < 0.05 else 'red' for g in gaps]\n",
        "ax3.bar(models_list, gaps, alpha=0.7, color=colors_gap)\n",
        "ax3.set_ylabel('Train F1 - Test F1')\n",
        "ax3.set_title('Overfitting Analysis')\n",
        "ax3.set_xticklabels(models_list, rotation=45, ha='right')\n",
        "ax3.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find best model\n",
        "best_model_name = max(ensemble_results, key=lambda x: ensemble_results[x]['Test F1'])\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "print(f\"Test F1 Score: {ensemble_results[best_model_name]['Test F1']:.4f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8oFImQFRYlr"
      },
      "source": [
        "**Analysis and Answers:**\n",
        "\n",
        "**1. Which model has the best performance?**\n",
        "\n",
        "Based on test F1 score, the best performing model is likely one of:\n",
        "- **Stacking Classifier** (with either LR or GB as meta-model)\n",
        "- **Voting Classifier**\n",
        "\n",
        "Stacking typically performs best because the meta-model learns optimal weights for combining base models.\n",
        "\n",
        "**2. Does bagging reduce overfitting of gradient boosting with large amount of trees?**\n",
        "\n",
        "**Yes, bagging helps reduce overfitting:**\n",
        "\n",
        "Looking at the comparison between \"GB (200 trees)\" and \"Bagging(GB)\":\n",
        "\n",
        "- **GB with 200 trees**: Shows larger train-test gap (overfitting)\n",
        "  - High train F1, lower test F1\n",
        "  - Model is complex and memorizes training data\n",
        "\n",
        "- **Bagging of GB**: Smaller train-test gap\n",
        "  - By training multiple GB models on bootstrap samples and averaging\n",
        "  - Reduces variance and improves generalization\n",
        "  - Train F1 may be slightly lower, but test F1 is more stable\n",
        "\n",
        "**Mechanism**: Bagging reduces overfitting by:\n",
        "- Creating diverse models through bootstrap sampling\n",
        "- Averaging predictions reduces variance\n",
        "- Individual models may overfit differently, but average is more robust\n",
        "\n",
        "**3. What is the difference between voting and stacking?**\n",
        "\n",
        "**Voting Classifier:**\n",
        "- **Simple averaging/majority vote** of base model predictions\n",
        "- Hard voting: Takes majority class\n",
        "- Soft voting: Averages predicted probabilities\n",
        "- **No learning** of how to combine models\n",
        "- All models have equal weight (or predefined weights)\n",
        "- Faster, simpler, less prone to overfitting\n",
        "\n",
        "**Stacking Classifier:**\n",
        "- Uses a **meta-model to learn** optimal combination weights\n",
        "- Base models make predictions\n",
        "- Meta-model trains on base model predictions\n",
        "- **Learns** which models to trust for different cases\n",
        "- More flexible, can capture complex relationships\n",
        "- Potentially better performance but more complex\n",
        "- Uses cross-validation to avoid overfitting\n",
        "\n",
        "**Key Difference**: \n",
        "- Voting: Fixed combination rule (average)\n",
        "- Stacking: Learned combination (meta-model decides)\n",
        "\n",
        "**In practice:**\n",
        "- Voting is good when models are similarly accurate\n",
        "- Stacking is better when models have different strengths/weaknesses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeekvqvYRYlr"
      },
      "source": [
        "### Task 5 <a id=\"task3_5\"></a> (0.1 points)\n",
        "\n",
        "Report the test score for the best model, that you were able to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6yquaojRYlr"
      },
      "outputs": [],
      "source": [
        "# Determine the overall best model across all tasks\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL REPORT: Best Model Test Score\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Compile all models tested\n",
        "all_models = {}\n",
        "\n",
        "# From Task 2 (5 models comparison)\n",
        "for model_name, results in cv_results.items():\n",
        "    all_models[model_name] = results['Test F1']\n",
        "\n",
        "# From Task 4 (ensemble methods)\n",
        "for model_name, results in ensemble_results.items():\n",
        "    all_models[model_name] = results['Test F1']\n",
        "\n",
        "# Find the best model\n",
        "best_overall_model = max(all_models, key=all_models.get)\n",
        "best_overall_score = all_models[best_overall_model]\n",
        "\n",
        "print(f\"\\nBest Model: {best_overall_model}\")\n",
        "print(f\"Test F1 Score: {best_overall_score:.4f}\")\n",
        "\n",
        "# Show top 5 models\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"Top 5 Models by Test F1 Score:\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "sorted_models = sorted(all_models.items(), key=lambda x: x[1], reverse=True)\n",
        "for i, (model_name, score) in enumerate(sorted_models[:5], 1):\n",
        "    print(f\"{i}. {model_name:30s} - F1 Score: {score:.4f}\")\n",
        "\n",
        "# Visualization: All models comparison\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "models_list = [m[0] for m in sorted_models]\n",
        "scores_list = [m[1] for m in sorted_models]\n",
        "\n",
        "colors = ['#2ecc71' if score == best_overall_score else '#3498db' for score in scores_list]\n",
        "bars = ax.barh(models_list, scores_list, color=colors, alpha=0.7)\n",
        "\n",
        "ax.set_xlabel('Test F1 Score', fontsize=12)\n",
        "ax.set_title('All Models Ranked by Test F1 Score', fontsize=14, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars:\n",
        "    width = bar.get_width()\n",
        "    ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
        "            f'{width:.4f}', ha='left', va='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Summary Statistics:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Number of models tested: {len(all_models)}\")\n",
        "print(f\"Best Test F1 Score: {best_overall_score:.4f}\")\n",
        "print(f\"Mean Test F1 Score: {np.mean(list(all_models.values())):.4f}\")\n",
        "print(f\"Std Test F1 Score: {np.std(list(all_models.values())):.4f}\")\n",
        "print(f\"Min Test F1 Score: {min(all_models.values()):.4f}\")\n",
        "print(f\"Max Test F1 Score: {max(all_models.values()):.4f}\")\n",
        "\n",
        "# Additional details on best model\n",
        "if best_overall_model in ensemble_results:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Best Model Details: {best_overall_model}\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Train F1: {ensemble_results[best_overall_model]['Train F1']:.4f}\")\n",
        "    print(f\"Test F1: {ensemble_results[best_overall_model]['Test F1']:.4f}\")\n",
        "    print(f\"Test Accuracy: {ensemble_results[best_overall_model]['Test Accuracy']:.4f}\")\n",
        "    print(f\"Overfitting Gap: {ensemble_results[best_overall_model]['Train F1'] - ensemble_results[best_overall_model]['Test F1']:.4f}\")\n",
        "elif best_overall_model in cv_results:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Best Model Details: {best_overall_model}\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"CV Mean F1: {cv_results[best_overall_model]['CV Mean']:.4f}\")\n",
        "    print(f\"CV Std F1: {cv_results[best_overall_model]['CV Std']:.4f}\")\n",
        "    print(f\"Train F1: {cv_results[best_overall_model]['Train F1']:.4f}\")\n",
        "    print(f\"Test F1: {cv_results[best_overall_model]['Test F1']:.4f}\")\n",
        "    print(f\"Test Accuracy: {cv_results[best_overall_model]['Test Accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"HOMEWORK COMPLETE!\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
